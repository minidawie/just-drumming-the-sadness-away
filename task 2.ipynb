{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f083a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import miditoolkit\n",
    "from miditoolkit import MidiFile, Instrument, Note\n",
    "import mido\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1981c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds sinusoidal positional encoding to token embeddings, following Vaswani et al.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "913d2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer-based model for sequence prediction using pitch only\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_encoder_layers=2, num_decoder_layers=2, dim_feedforward=512, max_len=1024, dropout=0.1):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = self.embedding(src) * np.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * np.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        src = src.permute(1, 0, 2)\n",
    "        tgt = tgt.permute(1, 0, 2)\n",
    "        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "        output = output.permute(1, 0, 2)\n",
    "        return self.output(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "584d1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the sparse categorical cross-entropy loss\n",
    "# Equivalent to nn.CrossEntropyLoss in PyTorch\n",
    "\n",
    "def get_loss():\n",
    "    return nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Generate a mask to prevent attention to future tokens\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e23a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset returns pitch-only sequences\n",
    "class MidiPitchDataset(Dataset):\n",
    "    def __init__(self, midi_files, seq_len=128):\n",
    "        self.data = []\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        for path in midi_files:\n",
    "            try:\n",
    "                midi_obj = miditoolkit.MidiFile(path, clip=True)\n",
    "                notes = []\n",
    "                for inst in midi_obj.instruments:\n",
    "                    if inst.is_drum and inst.notes:\n",
    "                        notes.extend(inst.notes)\n",
    "                if len(notes) < seq_len + 1:\n",
    "                    continue\n",
    "                notes = sorted(notes, key=lambda x: x.start)\n",
    "                pitch_seq = [note.pitch for note in notes]\n",
    "                for i in range(0, len(pitch_seq) - seq_len):\n",
    "                    src_seq = pitch_seq[i:i+seq_len]\n",
    "                    tgt_seq = pitch_seq[i+1:i+seq_len+1]\n",
    "                    self.data.append((src_seq, tgt_seq))\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {path.name} due to error: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        src_tensor = torch.tensor(src, dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor(tgt, dtype=torch.long)\n",
    "        return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7039025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, loss_fn, device, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}\")\n",
    "        for src, tgt in progress_bar:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_expected = tgt[:, 1:]\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            output = model(src, tgt_input, tgt_mask=tgt_mask)\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), tgt_expected.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "        print(f\"Epoch {epoch + 1} completed. Avg Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57cf12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_melody(model, seed_sequence, length=50, device='cpu'):\n",
    "    model.eval()\n",
    "    generated = seed_sequence[:]\n",
    "    valid_drums = [36, 38, 40, 41, 42, 43, 45, 46, 47, 48, 49, 50, 51, 53, 54, 55, 57, 59]\n",
    "    for _ in range(length):\n",
    "        src = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "        tgt = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "        if src.shape[1] > 128:\n",
    "            src = src[:, -128:]\n",
    "            tgt = tgt[:, -128:]\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src, tgt, tgt_mask=tgt_mask)\n",
    "            probs = torch.softmax(output[0, -1], dim=-1)\n",
    "            raw_pitch = torch.multinomial(probs, 1).item()\n",
    "            next_pitch = min(valid_drums, key=lambda x: abs(x - raw_pitch))\n",
    "        generated.append(next_pitch)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db29a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_melody_as_midi(pitch_sequence, file_path=\"generated_melody.mid\"):\n",
    "    from miditoolkit import MidiFile, Instrument, Note\n",
    "    midi = MidiFile()\n",
    "    instrument = Instrument(program=0, is_drum=True)\n",
    "    time = 0\n",
    "    for pitch in pitch_sequence:\n",
    "        note = Note(velocity=100, pitch=int(pitch), start=int(time), end=int(time + 120))\n",
    "        instrument.notes.append(note)\n",
    "        time += 120\n",
    "    midi.instruments.append(instrument)\n",
    "    midi.dump(file_path)\n",
    "    print(f\"Melody saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8400c73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1150 MIDI files.\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# Data loading and training\n",
    "# ========================\n",
    "midi_path = Path(\"groove\")\n",
    "midi_files = list(midi_path.rglob(\"*.mid\"))\n",
    "print(f\"Found {len(midi_files)} MIDI files.\")\n",
    "random.shuffle(midi_files)\n",
    "train_files = midi_files[:500]  # Limit to 500 for faster training\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = MidiPitchDataset(train_files)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model and training setup\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = MusicTransformer(vocab_size=128).to(device)\n",
    "optimizer = get_optimizer(model)\n",
    "loss_fn = get_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9492c9ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████| 4863/4863 [05:12<00:00, 15.58it/s, loss=1.54]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 1.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████| 4863/4863 [05:05<00:00, 15.90it/s, loss=1.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Avg Loss: 1.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████| 4863/4863 [05:06<00:00, 15.88it/s, loss=1.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Avg Loss: 1.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|████████████████████| 4863/4863 [05:08<00:00, 15.79it/s, loss=1.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Avg Loss: 1.2983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████| 4863/4863 [04:55<00:00, 16.43it/s, loss=1.27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Avg Loss: 1.2649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|███████████████████| 4863/4863 [04:52<00:00, 16.63it/s, loss=1.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Avg Loss: 1.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|███████████████████| 4863/4863 [04:53<00:00, 16.58it/s, loss=1.21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Avg Loss: 1.2091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|███████████████████| 4863/4863 [04:53<00:00, 16.59it/s, loss=1.18]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Avg Loss: 1.1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|███████████████████| 4863/4863 [04:54<00:00, 16.51it/s, loss=1.16]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Avg Loss: 1.1584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████████████| 4863/4863 [04:55<00:00, 16.46it/s, loss=1.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Avg Loss: 1.1355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_model(model, dataloader, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "94b380e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to music_transformer.pth\n"
     ]
    }
   ],
   "source": [
    "# Save model to disk\n",
    "model_path = \"music_transformer.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5998d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melody saved to generated_output.mid\n"
     ]
    }
   ],
   "source": [
    "# Get a sample directly from the dataset\n",
    "seed_src, _ = dataset[6050]  # Take the first source-target pair\n",
    "seed = seed_src[:32].tolist()  # Convert to list of ints\n",
    "generated = generate_melody(model, seed, length=64, device=device)\n",
    "save_melody_as_midi(generated, file_path=\"generated_output.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36105aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb404735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6cdf45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd08c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e82c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609bab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d8a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae792842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226b8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ccef87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65862f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a94377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e6b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722483b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
